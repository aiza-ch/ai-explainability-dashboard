# AI Model Explainability Dashboard

A visual dashboard to explain machine learning model decisions using **SHAP**, **LIME**, and model interpretation techniques.

## ğŸ“Š Project Summary
This project demonstrates **AI model explainability** using:
- **SHAP (SHapley Additive exPlanations)**
- **LIME (Local Interpretable Model-Agnostic Explanations)**

It allows users to:
- Select machine learning models (Logistic Regression, Random Forest)
- Visualize global and local explanations for predictions
- Compare model decision behaviors interactively

---

## ğŸš€ Key Features
- Interactive model selection (Logistic Regression, Random Forest)
- SHAP global summary plots
- SHAP local waterfall plots
- LIME local explanations
- Clean, user-friendly Streamlit dashboard

---

## ğŸ› ï¸ Tech Stack
- Python
- Streamlit
- SHAP
- LIME
- Scikit-learn
- Matplotlib
- Pandas

---

## ğŸ“‚ Project Structure
```text
ai-explainability-dashboard/
â”œâ”€â”€ data/              # Dataset files
â”œâ”€â”€ dashboard/         # Streamlit dashboard app
â”œâ”€â”€ models/            # Saved machine learning models
â”œâ”€â”€ notebooks/         # Optional Jupyter notebooks for experiments
â”œâ”€â”€ src/               # Data loading, training, SHAP & LIME logic
â”œâ”€â”€ utils/             # (Optional) Helper functions
â”œâ”€â”€ requirements.txt   # Project dependencies
â”œâ”€â”€ README.md          # Project documentation



# Clone the repository
git clone https://github.com/aiza-ch/ai-explainability-dashboard.git
cd ai-explainability-dashboard

# Install dependencies
pip install -r requirements.txt

# Run the Streamlit app
streamlit run dashboard/app.py
